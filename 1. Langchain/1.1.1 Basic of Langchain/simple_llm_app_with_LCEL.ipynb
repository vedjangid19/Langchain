{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build A simple LLM Application with LCEL\n",
    "\n",
    "1. Using Language Model\n",
    "2. Using PromptTemplates and OutputParsers\n",
    "3. Using Langchain Expression Language(LCEL) to chain components togeather\n",
    "4. Debugging and Tracing your Application using Langsmith\n",
    "5. Deploying your application with langserve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001F841DF6620>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001F841DF5660>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatGroq(\n",
    "    model=\"gemma2-9b-it\",\n",
    "    groq_api_key=groq_api_key\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Bonjour, Comment allez-vous ? \\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 34, 'total_tokens': 45, 'completion_time': 0.02, 'prompt_time': 0.000308109, 'queue_time': 0.014099831, 'total_time': 0.020308109}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-851a0e36-4830-417a-934f-0c660e6c6cbe-0', usage_metadata={'input_tokens': 34, 'output_tokens': 11, 'total_tokens': 45})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"Translate the following from english to french, make sure please don't give any other extra informations.\"),\n",
    "    HumanMessage(content=\"Hello How are you?\")\n",
    "]\n",
    "\n",
    "response1 = model.invoke(messages)\n",
    "response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bonjour, Comment allez-vous ? \\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "parser.invoke(response1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using LCEL We can chain the components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bonjour, comment allez-vous ? \\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = model| parser\n",
    "\n",
    "response_with_chain = chain.invoke(messages)\n",
    "response_with_chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bonjour  (formal)\\n\\nSalut (informal)\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "generaic_template = \"Translate the following into {language}:\"\n",
    "text = \"Hello\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", generaic_template),\n",
    "        (\"user\", \"{text}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chat_template_response = chain.invoke({\"language\": \"French\", \"text\": text})\n",
    "chat_template_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'नमस्ते (Namaste) \\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_template_response2 = chain.invoke({\"language\": \"Hindi\", \"text\": text})\n",
    "chat_template_response2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
